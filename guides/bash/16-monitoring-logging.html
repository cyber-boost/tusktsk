<h1>Monitoring & Logging</h1>

<h2>Monitoring and Logging with TuskLang in Bash</h2>
<p>Implement comprehensive monitoring, logging, alerting, and observability for bash applications.</p>

<h3>monitoring.tsk</h3>
<pre>
# Monitoring configuration
monitoring {
    # Logging configuration
    logging: {
        # Log levels
        levels: {
            trace: 0
            debug: 1
            info: 2
            warn: 3
            error: 4
            fatal: 5
        }
        
        # Current log level
        level: env("LOG_LEVEL", "info")
        
        # Log destinations
        destinations: {
            console: {
                enabled: true
                format: @logFormat.console
                level: "debug"
                
                colors: {
                    trace: "dim"
                    debug: "gray"
                    info: "blue"
                    warn: "yellow"
                    error: "red"
                    fatal: ["red", "bold"]
                }
            }
            
            file: {
                enabled: true
                path: "./logs/app.log"
                format: @logFormat.json
                level: "info"
                
                rotation: {
                    enabled: true
                    max_size: "100MB"
                    max_files: 10
                    compress: true
                }
            }
            
            syslog: {
                enabled: @env.PRODUCTION
                facility: "user"
                format: @logFormat.syslog
                level: "warn"
            }
            
            remote: {
                enabled: @env.LOG_ENDPOINT != null
                endpoint: @env.LOG_ENDPOINT
                format: @logFormat.json
                level: "error"
                
                auth: {
                    type: "bearer"
                    token: @env.LOG_TOKEN
                }
                
                batch: {
                    size: 100
                    timeout: "5s"
                }
            }
        }
        
        # Log formats
        formats: {
            console: "${timestamp} [${level}] ${component}: ${message}"
            json: {
                timestamp: @timestamp(),
                level: @level,
                component: @component,
                message: @message,
                context: @context,
                trace_id: @trace.id,
                span_id: @span.id
            }
            syslog: "<${priority}>${timestamp} ${hostname} ${component}[${pid}]: ${message}"
        }
        
        # Structured logging context
        context: {
            service: @app.name,
            version: @app.version,
            environment: @env.ENVIRONMENT,
            hostname: @system.hostname,
            pid: @process.pid
        }
    }
    
    # Metrics collection
    metrics: {
        # System metrics
        system: {
            enabled: true
            interval: "10s"
            
            collectors: {
                cpu: {
                    enabled: true
                    metrics: ["usage", "load", "cores"]
                }
                
                memory: {
                    enabled: true
                    metrics: ["used", "available", "buffers", "cached"]
                }
                
                disk: {
                    enabled: true
                    metrics: ["usage", "io_read", "io_write", "latency"]
                    paths: ["/", "/var", "/tmp"]
                }
                
                network: {
                    enabled: true
                    metrics: ["bytes_in", "bytes_out", "packets", "errors"]
                    interfaces: ["eth0", "lo"]
                }
                
                processes: {
                    enabled: true
                    metrics: ["count", "threads", "fd_count"]
                }
            }
        }
        
        # Application metrics
        application: {
            enabled: true
            
            # Custom metrics
            counters: {
                requests_total: "Total HTTP requests"
                errors_total: "Total errors"
                tasks_completed: "Completed tasks"
            }
            
            gauges: {
                active_connections: "Active connections"
                queue_size: "Queue size"
                memory_usage: "Memory usage"
            }
            
            histograms: {
                request_duration: {
                    description: "Request duration in seconds"
                    buckets: [0.1, 0.25, 0.5, 1.0, 2.5, 5.0, 10.0]
                }
                
                task_duration: {
                    description: "Task processing time"
                    buckets: [0.01, 0.1, 1.0, 10.0, 60.0]
                }
            }
        }
        
        # Export configuration
        export: {
            prometheus: {
                enabled: true
                endpoint: "/metrics"
                port: 9090
                
                labels: {
                    service: @app.name
                    version: @app.version
                    environment: @env.ENVIRONMENT
                }
            }
            
            statsd: {
                enabled: @env.STATSD_HOST != null
                host: @env.STATSD_HOST
                port: @env.STATSD_PORT || 8125
                prefix: @app.name
            }
        }
    }
    
    # Health checks
    health: {
        enabled: true
        endpoint: "/health"
        
        checks: {
            # Basic checks
            basic: {
                uptime: @system.uptime
                memory: @system.memory.available > 100MB
                disk: @system.disk.free > 1GB
            }
            
            # Service dependencies
            dependencies: {
                database: @healthCheck.database({
                    timeout: "5s"
                    query: "SELECT 1"
                })
                
                redis: @healthCheck.redis({
                    timeout: "2s"
                    command: "ping"
                })
                
                external_api: @healthCheck.http({
                    url: @env.API_ENDPOINT + "/health"
                    timeout: "10s"
                    expected_status: 200
                })
            }
        }
        
        # Overall health status
        status: @all(@health.checks) ? "healthy" : "unhealthy"
        
        # Readiness check
        readiness: {
            enabled: true
            endpoint: "/ready"
            
            checks: {
                database_migration: @db.migrationComplete
                config_loaded: @config.isLoaded
                cache_warmed: @cache.isWarmed
            }
        }
    }
    
    # Alerting configuration
    alerting: {
        enabled: true
        
        # Alert rules
        rules: {
            high_cpu_usage: {
                condition: @metrics.system.cpu.usage > 90
                duration: "5m"
                severity: "warning"
                message: "High CPU usage detected: ${@metrics.system.cpu.usage}%"
                
                actions: ["slack", "email"]
                runbook: "https://wiki.example.com/runbooks/high-cpu"
            }
            
            high_memory_usage: {
                condition: @metrics.system.memory.used_percent > 95
                duration: "2m"
                severity: "critical"
                message: "Critical memory usage: ${@metrics.system.memory.used_percent}%"
                
                actions: ["slack", "pagerduty"]
            }
            
            high_error_rate: {
                condition: @metrics.application.error_rate > 0.05
                duration: "1m"
                severity: "warning"
                message: "High error rate: ${@metrics.application.error_rate * 100}%"
                
                actions: ["slack"]
            }
            
            service_down: {
                condition: @health.status == "unhealthy"
                duration: "30s"
                severity: "critical"
                message: "Service health check failed"
                
                actions: ["slack", "pagerduty", "webhook"]
            }
        }
        
        # Notification channels
        channels: {
            slack: {
                webhook_url: @env.SLACK_WEBHOOK
                channel: "#alerts"
                username: "TuskLang Monitor"
                
                template: {
                    title: "${severity}: ${rule_name}"
                    text: "${message}"
                    color: @severity == "critical" ? "danger" : "warning"
                }
            }
            
            email: {
                smtp_host: @env.SMTP_HOST
                smtp_port: @env.SMTP_PORT || 587
                username: @env.SMTP_USER
                password: @env.SMTP_PASS
                
                from: "alerts@example.com"
                to: ["oncall@example.com"]
                
                template: @template.email
            }
            
            pagerduty: {
                integration_key: @env.PAGERDUTY_KEY
                
                template: {
                    summary: "${message}"
                    severity: @severity
                    component: @app.name
                    group: "infrastructure"
                }
            }
        }
    }
    
    # Tracing configuration
    tracing: {
        enabled: @env.TRACING_ENABLED || false
        
        # Sampling
        sampling: {
            strategy: "probabilistic"
            rate: 0.1  # 10% of traces
            
            # Always sample for errors
            always_sample: {
                on_error: true
                on_slow_request: 1000  # ms
            }
        }
        
        # Exporters
        exporters: {
            jaeger: {
                enabled: @env.JAEGER_ENDPOINT != null
                endpoint: @env.JAEGER_ENDPOINT
                
                tags: {
                    service: @app.name
                    version: @app.version
                }
            }
            
            zipkin: {
                enabled: @env.ZIPKIN_ENDPOINT != null
                endpoint: @env.ZIPKIN_ENDPOINT
            }
        }
    }
}

# Dashboard configuration
dashboards {
    grafana: {
        enabled: true
        
        # Dashboard templates
        system: {
            title: "System Metrics"
            
            panels: [
                {
                    title: "CPU Usage"
                    type: "graph"
                    query: "system_cpu_usage"
                    threshold: 80
                },
                {
                    title: "Memory Usage"
                    type: "singlestat"
                    query: "system_memory_used_percent"
                    unit: "percent"
                },
                {
                    title: "Disk I/O"
                    type: "graph"
                    query: "system_disk_io_bytes"
                    unit: "bytes/sec"
                }
            ]
        }
        
        application: {
            title: "Application Metrics"
            
            panels: [
                {
                    title: "Request Rate"
                    type: "graph"
                    query: "rate(requests_total[5m])"
                    unit: "rps"
                },
                {
                    title: "Error Rate"
                    type: "graph"
                    query: "rate(errors_total[5m]) / rate(requests_total[5m])"
                    unit: "percent"
                },
                {
                    title: "Response Time"
                    type: "histogram"
                    query: "request_duration_seconds"
                    unit: "seconds"
                }
            ]
        }
    }
}
</pre>

<h3>Bash Monitoring Implementation</h3>
<pre>
#!/bin/bash

# TuskLang Monitoring & Logging Library

# Global monitoring state
declare -A LOG_LEVELS=([trace]=0 [debug]=1 [info]=2 [warn]=3 [error]=4 [fatal]=5)
declare -A METRICS
declare -A ALERTS
declare -A HEALTH_CHECKS

LOG_LEVEL_NAME=${LOG_LEVEL:-info}
LOG_LEVEL_NUM=${LOG_LEVELS[$LOG_LEVEL_NAME]}
LOG_DIR="./logs"
METRICS_DIR="./metrics"

# Initialize monitoring
tusk_monitor_init() {
    # Create directories
    mkdir -p "$LOG_DIR" "$METRICS_DIR"
    
    # Initialize metrics
    METRICS[start_time]=$(date +%s)
    METRICS[requests_total]=0
    METRICS[errors_total]=0
    
    # Start background monitoring
    tusk_monitor_start_collectors &
    MONITOR_PID=$!
    
    # Set up signal handlers
    trap 'tusk_monitor_cleanup' EXIT INT TERM
}

# Logging functions
tusk_log() {
    local level="$1"
    local component="${2:-main}"
    local message="$3"
    local context="${4:-{}}"
    
    # Check log level
    local level_num=${LOG_LEVELS[$level]:-2}
    [ $level_num -lt $LOG_LEVEL_NUM ] && return
    
    local timestamp=$(date -u +"%Y-%m-%dT%H:%M:%SZ")
    local pid=$$
    
    # Console output
    tusk_log_console "$timestamp" "$level" "$component" "$message"
    
    # File output
    tusk_log_file "$timestamp" "$level" "$component" "$message" "$context"
    
    # Remote logging for errors
    [ "$level" = "error" ] || [ "$level" = "fatal" ] && \
        tusk_log_remote "$timestamp" "$level" "$component" "$message" "$context"
}

tusk_log_console() {
    local timestamp="$1"
    local level="$2"
    local component="$3"
    local message="$4"
    
    # Color based on level
    local color=""
    case "$level" in
        trace) color="\033[2m" ;;     # dim
        debug) color="\033[90m" ;;    # gray
        info)  color="\033[34m" ;;    # blue
        warn)  color="\033[33m" ;;    # yellow
        error) color="\033[31m" ;;    # red
        fatal) color="\033[31;1m" ;;  # bold red
    esac
    
    # Format output
    local formatted="${timestamp} ${color}[${level^^}]\033[0m ${component}: ${message}"
    
    if [ "$level" = "error" ] || [ "$level" = "fatal" ]; then
        echo -e "$formatted" >&2
    else
        echo -e "$formatted"
    fi
}

tusk_log_file() {
    local timestamp="$1"
    local level="$2"
    local component="$3"
    local message="$4"
    local context="$5"
    
    # JSON format for structured logging
    local log_entry=$(cat <<EOF
{
    "timestamp": "$timestamp",
    "level": "$level",
    "component": "$component",
    "message": "$message",
    "context": $context,
    "hostname": "$(hostname)",
    "pid": $$,
    "trace_id": "${TRACE_ID:-}",
    "span_id": "${SPAN_ID:-}"
}
EOF
    )
    
    echo "$log_entry" >> "$LOG_DIR/app.log"
    
    # Rotate logs if needed
    tusk_log_rotate "$LOG_DIR/app.log"
}

tusk_log_remote() {
    local timestamp="$1"
    local level="$2"
    local component="$3"
    local message="$4"
    local context="$5"
    
    # Send to remote logging service
    if [ -n "${LOG_ENDPOINT:-}" ]; then
        local payload=$(cat <<EOF
{
    "timestamp": "$timestamp",
    "level": "$level",
    "service": "${APP_NAME:-unknown}",
    "component": "$component",
    "message": "$message",
    "context": $context
}
EOF
        )
        
        curl -s -X POST "$LOG_ENDPOINT" \
            -H "Content-Type: application/json" \
            -H "Authorization: Bearer ${LOG_TOKEN:-}" \
            -d "$payload" &
    fi
}

tusk_log_rotate() {
    local log_file="$1"
    local max_size=$((100 * 1024 * 1024))  # 100MB
    
    if [ -f "$log_file" ]; then
        local size=$(stat -f%z "$log_file" 2>/dev/null || stat -c%s "$log_file")
        
        if [ "$size" -gt "$max_size" ]; then
            # Rotate logs
            mv "$log_file" "${log_file}.$(date +%Y%m%d_%H%M%S)"
            
            # Keep only last 10 rotated files
            ls -t "${log_file}".* 2>/dev/null | tail -n +11 | xargs rm -f
        fi
    fi
}

# Convenience logging functions
tusk_log_trace() { tusk_log "trace" "${1:-main}" "$2" "${3:-{}}"; }
tusk_log_debug() { tusk_log "debug" "${1:-main}" "$2" "${3:-{}}"; }
tusk_log_info() { tusk_log "info" "${1:-main}" "$2" "${3:-{}}"; }
tusk_log_warn() { tusk_log "warn" "${1:-main}" "$2" "${3:-{}}"; }
tusk_log_error() { tusk_log "error" "${1:-main}" "$2" "${3:-{}}"; }
tusk_log_fatal() { tusk_log "fatal" "${1:-main}" "$2" "${3:-{}}"; }

# Metrics collection
tusk_metric_counter() {
    local name="$1"
    local value="${2:-1}"
    local labels="$3"
    
    METRICS[$name]=$((${METRICS[$name]:-0} + value))
    
    # Export to Prometheus format
    echo "# TYPE $name counter" >> "$METRICS_DIR/metrics.prom"
    echo "${name}{${labels:-}} ${METRICS[$name]}" >> "$METRICS_DIR/metrics.prom"
}

tusk_metric_gauge() {
    local name="$1"
    local value="$2"
    local labels="$3"
    
    METRICS[$name]="$value"
    
    # Export to Prometheus format
    echo "# TYPE $name gauge" >> "$METRICS_DIR/metrics.prom"
    echo "${name}{${labels:-}} $value" >> "$METRICS_DIR/metrics.prom"
}

tusk_metric_histogram() {
    local name="$1"
    local value="$2"
    local labels="$3"
    
    # Simple histogram implementation
    local buckets=(0.1 0.25 0.5 1.0 2.5 5.0 10.0)
    
    for bucket in "${buckets[@]}"; do
        if (( $(echo "$value <= $bucket" | bc -l) )); then
            local counter_name="${name}_bucket"
            local bucket_labels="le=\"$bucket\""
            [ -n "$labels" ] && bucket_labels="$labels,$bucket_labels"
            
            tusk_metric_counter "$counter_name" 1 "$bucket_labels"
        fi
    done
    
    # Total count and sum
    tusk_metric_counter "${name}_count" 1 "$labels"
    tusk_metric_counter "${name}_sum" "$value" "$labels"
}

# System metrics collection
tusk_collect_system_metrics() {
    # CPU usage
    local cpu_usage=$(top -bn1 | grep "Cpu(s)" | awk '{print $2}' | sed 's/%us,//')
    tusk_metric_gauge "system_cpu_usage_percent" "$cpu_usage"
    
    # Memory usage
    local mem_total=$(free -b | awk '/^Mem:/ {print $2}')
    local mem_used=$(free -b | awk '/^Mem:/ {print $3}')
    local mem_percent=$(echo "scale=2; $mem_used * 100 / $mem_total" | bc)
    
    tusk_metric_gauge "system_memory_total_bytes" "$mem_total"
    tusk_metric_gauge "system_memory_used_bytes" "$mem_used"
    tusk_metric_gauge "system_memory_used_percent" "$mem_percent"
    
    # Disk usage
    df -B1 | awk 'NR>1 {print $1,$2,$3,$5,$6}' | while read filesystem total used percent mount; do
        local labels="filesystem=\"$filesystem\",mount=\"$mount\""
        tusk_metric_gauge "system_disk_total_bytes" "$total" "$labels"
        tusk_metric_gauge "system_disk_used_bytes" "$used" "$labels"
    done
    
    # Load average
    local load=$(uptime | awk -F'load average:' '{print $2}' | awk -F, '{print $1}' | tr -d ' ')
    tusk_metric_gauge "system_load_1m" "$load"
    
    # Process count
    local proc_count=$(ps aux | wc -l)
    tusk_metric_gauge "system_processes_total" "$proc_count"
}

# Health checks
tusk_health_check() {
    local name="$1"
    local check_function="$2"
    local timeout="${3:-5}"
    
    local start_time=$(date +%s%N)
    local result=0
    
    # Run check with timeout
    if timeout "$timeout" bash -c "$check_function" >/dev/null 2>&1; then
        result=1
        tusk_log_debug "health" "Check $name passed"
    else
        result=0
        tusk_log_warn "health" "Check $name failed"
    fi
    
    local end_time=$(date +%s%N)
    local duration=$(echo "scale=3; ($end_time - $start_time) / 1000000000" | bc)
    
    # Store result
    HEALTH_CHECKS[$name]="$result"
    
    # Export metrics
    tusk_metric_gauge "health_check_status" "$result" "check=\"$name\""
    tusk_metric_histogram "health_check_duration_seconds" "$duration" "check=\"$name\""
    
    return $((1 - result))
}

# Built-in health checks
tusk_health_check_disk_space() {
    local path="${1:-/}"
    local min_free="${2:-1048576}"  # 1GB in KB
    
    local available=$(df -k "$path" | awk 'NR==2 {print $4}')
    [ "$available" -gt "$min_free" ]
}

tusk_health_check_memory() {
    local min_free="${1:-104857600}"  # 100MB in bytes
    
    local available=$(free -b | awk '/^Mem:/ {print $7}')
    [ "$available" -gt "$min_free" ]
}

tusk_health_check_port() {
    local port="$1"
    local host="${2:-localhost}"
    
    nc -z "$host" "$port"
}

tusk_health_check_url() {
    local url="$1"
    local expected_status="${2:-200}"
    
    local status=$(curl -s -o /dev/null -w "%{http_code}" "$url")
    [ "$status" = "$expected_status" ]
}

# Alerting
tusk_alert_check() {
    local rule_name="$1"
    local condition="$2"
    local message="$3"
    local severity="${4:-warning}"
    
    if eval "$condition"; then
        # Check if already alerted
        local alert_file="$METRICS_DIR/alert_${rule_name}"
        
        if [ ! -f "$alert_file" ]; then
            # New alert
            echo "$(date +%s)" > "$alert_file"
            tusk_alert_fire "$rule_name" "$message" "$severity"
            tusk_log_warn "alerting" "Alert fired: $rule_name - $message"
        fi
    else
        # Condition resolved
        local alert_file="$METRICS_DIR/alert_${rule_name}"
        if [ -f "$alert_file" ]; then
            rm -f "$alert_file"
            tusk_alert_resolve "$rule_name"
            tusk_log_info "alerting" "Alert resolved: $rule_name"
        fi
    fi
}

tusk_alert_fire() {
    local rule_name="$1"
    local message="$2"
    local severity="$3"
    
    # Send to configured channels
    [ -n "${SLACK_WEBHOOK:-}" ] && tusk_alert_slack "$rule_name" "$message" "$severity"
    [ -n "${EMAIL_TO:-}" ] && tusk_alert_email "$rule_name" "$message" "$severity"
    
    # Metrics
    tusk_metric_counter "alerts_total" 1 "rule=\"$rule_name\",severity=\"$severity\""
}

tusk_alert_slack() {
    local rule_name="$1"
    local message="$2"
    local severity="$3"
    
    local color="warning"
    [ "$severity" = "critical" ] && color="danger"
    
    local payload=$(cat <<EOF
{
    "channel": "#alerts",
    "username": "TuskLang Monitor",
    "attachments": [{
        "color": "$color",
        "title": "$severity: $rule_name",
        "text": "$message",
        "ts": $(date +%s)
    }]
}
EOF
    )
    
    curl -s -X POST "$SLACK_WEBHOOK" \
        -H "Content-Type: application/json" \
        -d "$payload" &
}

tusk_alert_email() {
    local rule_name="$1"
    local message="$2"
    local severity="$3"
    
    local subject="[$severity] Alert: $rule_name"
    local body="Alert: $rule_name\n\nMessage: $message\n\nTime: $(date)\nHost: $(hostname)"
    
    echo -e "$body" | mail -s "$subject" "${EMAIL_TO}"
}

# Tracing
tusk_trace_start() {
    local operation="$1"
    
    TRACE_ID=$(openssl rand -hex 16)
    SPAN_ID=$(openssl rand -hex 8)
    SPAN_START=$(date +%s%N)
    
    tusk_log_debug "tracing" "Started trace for $operation" "{\"trace_id\":\"$TRACE_ID\"}"
}

tusk_trace_end() {
    local operation="$1"
    
    local span_end=$(date +%s%N)
    local duration=$(echo "scale=6; ($span_end - $SPAN_START) / 1000000000" | bc)
    
    tusk_log_debug "tracing" "Completed trace for $operation" \
        "{\"trace_id\":\"$TRACE_ID\",\"duration\":$duration}"
    
    # Send to tracing backend
    if [ -n "${JAEGER_ENDPOINT:-}" ]; then
        tusk_trace_export "$operation" "$duration"
    fi
    
    unset TRACE_ID SPAN_ID SPAN_START
}

# Monitoring loop
tusk_monitor_start_collectors() {
    while true; do
        # Collect system metrics
        tusk_collect_system_metrics
        
        # Run health checks
        tusk_health_check "disk_space" "tusk_health_check_disk_space /"
        tusk_health_check "memory" "tusk_health_check_memory"
        
        # Check alerts
        local cpu_usage="${METRICS[system_cpu_usage_percent]:-0}"
        tusk_alert_check "high_cpu" "[ \"$cpu_usage\" -gt 90 ]" \
            "High CPU usage: ${cpu_usage}%" "warning"
        
        local mem_percent="${METRICS[system_memory_used_percent]:-0}"
        tusk_alert_check "high_memory" \
            "[ \"\$(echo \"$mem_percent > 95\" | bc)\" = \"1\" ]" \
            "High memory usage: ${mem_percent}%" "critical"
        
        # Export metrics
        > "$METRICS_DIR/metrics.prom"
        
        sleep 10
    done
}

# Cleanup
tusk_monitor_cleanup() {
    [ -n "$MONITOR_PID" ] && kill "$MONITOR_PID" 2>/dev/null || true
    tusk_log_info "monitoring" "Monitoring system stopped"
}

# HTTP metrics endpoint
tusk_metrics_endpoint() {
    local port="${1:-9090}"
    
    while true; do
        {
            echo -e "HTTP/1.1 200 OK\r"
            echo -e "Content-Type: text/plain\r"
            echo -e "\r"
            
            # Add timestamp
            echo "# TYPE tusk_scrape_timestamp gauge"
            echo "tusk_scrape_timestamp $(date +%s)"
            echo
            
            # Output metrics
            cat "$METRICS_DIR/metrics.prom" 2>/dev/null || echo "# No metrics available"
        } | nc -l -p "$port" -q 1
    done
}
</pre>

<h3>Usage Example</h3>
<pre>
#!/bin/bash

# Load monitoring library
source /usr/local/lib/tusklang-monitoring.sh

# Initialize monitoring
tusk_monitor_init

echo "=== Monitoring & Logging Demo ==="

# 1. Basic logging
echo -e "\n1. Logging Examples:"

tusk_log_info "main" "Application started"
tusk_log_debug "config" "Loading configuration file" '{"file":"app.config"}'
tusk_log_warn "cache" "Cache miss for key: user_123"
tusk_log_error "database" "Connection failed" '{"error":"timeout","duration":30}'

# 2. Metrics collection
echo -e "\n2. Metrics Collection:"

# Counter metrics
tusk_metric_counter "http_requests_total" 1 'method="GET",status="200"'
tusk_metric_counter "http_requests_total" 1 'method="POST",status="201"'
tusk_metric_counter "errors_total" 1 'type="validation"'

# Gauge metrics
tusk_metric_gauge "active_connections" 25
tusk_metric_gauge "queue_size" 150
tusk_metric_gauge "temperature_celsius" 72.5

# Histogram metrics
tusk_metric_histogram "request_duration_seconds" 0.245
tusk_metric_histogram "request_duration_seconds" 1.123
tusk_metric_histogram "request_duration_seconds" 0.089

echo "Metrics collected!"

# 3. Health checks
echo -e "\n3. Health Checks:"

# Built-in health checks
echo -n "Disk space check: "
if tusk_health_check "disk_space" "tusk_health_check_disk_space /"; then
    echo "✓ Passed"
else
    echo "✗ Failed"
fi

echo -n "Memory check: "
if tusk_health_check "memory" "tusk_health_check_memory"; then
    echo "✓ Passed"
else
    echo "✗ Failed"
fi

# Custom health check
check_service_status() {
    # Simulate service check
    [ $((RANDOM % 4)) -ne 0 ]  # 75% success rate
}

echo -n "Service status check: "
if tusk_health_check "service" "check_service_status"; then
    echo "✓ Passed"
else
    echo "✗ Failed"
fi

# 4. Tracing
echo -e "\n4. Distributed Tracing:"

process_request() {
    local request_id="$1"
    
    tusk_trace_start "process_request"
    
    tusk_log_info "api" "Processing request $request_id" \
        "{\"request_id\":\"$request_id\"}"
    
    # Simulate processing
    sleep 0.5
    
    # Log some steps
    tusk_log_debug "validation" "Request validated"
    tusk_log_debug "business" "Business logic executed"
    tusk_log_info "response" "Request completed successfully"
    
    tusk_trace_end "process_request"
}

process_request "req_12345"

# 5. Alerting simulation
echo -e "\n5. Alerting System:"

# Simulate high CPU usage
METRICS[system_cpu_usage_percent]=95

echo "Simulating high CPU usage (95%)..."
tusk_alert_check "high_cpu" "[ \"${METRICS[system_cpu_usage_percent]}\" -gt 90 ]" \
    "High CPU usage: ${METRICS[system_cpu_usage_percent]}%" "warning"

# Simulate resolution
METRICS[system_cpu_usage_percent]=45

echo "CPU usage normalized (45%)..."
tusk_alert_check "high_cpu" "[ \"${METRICS[system_cpu_usage_percent]}\" -gt 90 ]" \
    "High CPU usage: ${METRICS[system_cpu_usage_percent]}%" "warning"

# 6. Structured logging with context
echo -e "\n6. Structured Logging:"

# Create a request context
export REQUEST_ID="req_$(openssl rand -hex 8)"
export USER_ID="user_12345"

log_with_context() {
    local level="$1"
    local component="$2"
    local message="$3"
    
    local context=$(cat <<EOF
{
    "request_id": "$REQUEST_ID",
    "user_id": "$USER_ID",
    "session_id": "sess_abcdef",
    "ip_address": "192.168.1.100",
    "user_agent": "TuskLang/1.0"
}
EOF
    )
    
    tusk_log "$level" "$component" "$message" "$context"
}

log_with_context "info" "auth" "User login successful"
log_with_context "debug" "session" "Session created"
log_with_context "warn" "rate_limit" "Rate limit threshold reached"

# 7. Performance monitoring
echo -e "\n7. Performance Monitoring:"

monitor_function() {
    local func_name="$1"
    shift
    
    local start_time=$(date +%s%N)
    
    "$@"
    local exit_code=$?
    
    local end_time=$(date +%s%N)
    local duration=$(echo "scale=6; ($end_time - $start_time) / 1000000000" | bc)
    
    # Log performance
    tusk_log_debug "performance" "Function $func_name completed in ${duration}s"
    
    # Record metrics
    tusk_metric_histogram "function_duration_seconds" "$duration" "function=\"$func_name\""
    tusk_metric_counter "function_calls_total" 1 "function=\"$func_name\""
    
    return $exit_code
}

slow_function() {
    echo "Doing complex work..."
    sleep 1
    return 0
}

monitor_function "slow_function" slow_function

# 8. Log aggregation example
echo -e "\n8. Log Analysis:"

# Show recent logs
echo "Recent application logs:"
tail -5 "$LOG_DIR/app.log" 2>/dev/null | while IFS= read -r line; do
    echo "  $line"
done

# Show metrics
echo -e "\nCurrent metrics:"
echo "  Total requests: ${METRICS[http_requests_total]:-0}"
echo "  Total errors: ${METRICS[errors_total]:-0}"
echo "  Active connections: ${METRICS[active_connections]:-0}"

# Show health status
echo -e "\nHealth check status:"
for check in "${!HEALTH_CHECKS[@]}"; do
    status="${HEALTH_CHECKS[$check]}"
    if [ "$status" = "1" ]; then
        echo "  ✓ $check: healthy"
    else
        echo "  ✗ $check: unhealthy"
    fi
done

# 9. Prometheus metrics export
echo -e "\n9. Metrics Export:"

# Start metrics endpoint in background
tusk_metrics_endpoint 9090 &
METRICS_PID=$!

# Wait a moment and test
sleep 1

echo "Metrics available at http://localhost:9090/metrics"
echo "Sample metrics output:"
curl -s http://localhost:9090/metrics | head -10

# Cleanup
kill $METRICS_PID 2>/dev/null || true

# 10. Dashboard data
echo -e "\n10. Dashboard Integration:"

# Generate dashboard JSON
cat > dashboard.json << 'EOF'
{
    "dashboard": {
        "title": "TuskLang Application Metrics",
        "panels": [
            {
                "title": "Request Rate",
                "type": "graph",
                "targets": [
                    {
                        "expr": "rate(http_requests_total[5m])",
                        "legendFormat": "{{method}} {{status}}"
                    }
                ]
            },
            {
                "title": "Error Rate",
                "type": "singlestat",
                "targets": [
                    {
                        "expr": "rate(errors_total[5m]) / rate(http_requests_total[5m]) * 100",
                        "legendFormat": "Error Rate %"
                    }
                ]
            }
        ]
    }
}
EOF

echo "Dashboard configuration generated: dashboard.json"

# Cleanup
rm -f dashboard.json

echo -e "\n=== Monitoring demonstration complete ==="
</pre>

<p>TuskLang monitoring and logging in bash provides enterprise-grade observability with structured logging, metrics collection, health checks, alerting, and distributed tracing capabilities.</p>