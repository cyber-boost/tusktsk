<h1>Testing & Debugging</h1>

<h2>Testing and Debugging TuskLang Applications in Bash</h2>
<p>Comprehensive testing frameworks, debugging tools, and quality assurance for TuskLang bash applications.</p>

<h3>testing.tsk</h3>
<pre>
# Testing configuration
testing {
    # Test framework configuration
    framework: {
        name: "tusk-test"
        version: "1.0.0"
        
        # Test discovery
        discovery: {
            patterns: ["**/*.test.sh", "**/*.spec.sh", "tests/**/*.sh"]
            exclude: ["node_modules/**", "vendor/**"]
            
            # Test naming conventions
            conventions: {
                test_prefix: "test_"
                setup_function: "setup"
                teardown_function: "teardown"
                before_each: "before_each"
                after_each: "after_each"
            }
        }
        
        # Test execution
        execution: {
            parallel: true
            max_workers: @cpu.cores
            timeout: "30s"
            
            # Test isolation
            isolation: {
                temp_dir: "./tmp/tests"
                clean_env: true
                sandbox: true
            }
        }
        
        # Reporting
        reporting: {
            formats: ["console", "junit", "json", "html"]
            output_dir: "./test-results"
            
            # Coverage reporting
            coverage: {
                enabled: true
                threshold: 80
                formats: ["text", "html", "cobertura"]
                exclude: ["tests/**", "vendor/**"]
            }
        }
    }
    
    # Test types
    types: {
        unit: {
            description: "Unit tests for individual functions"
            pattern: "**/*.unit.test.sh"
            
            setup: {
                mock_external_commands: true
                isolated_filesystem: true
            }
            
            assertions: [
                "assert_equals",
                "assert_not_equals", 
                "assert_contains",
                "assert_matches",
                "assert_true",
                "assert_false",
                "assert_null",
                "assert_not_null",
                "assert_exits_with",
                "assert_output_contains"
            ]
        }
        
        integration: {
            description: "Integration tests for component interaction"
            pattern: "**/*.integration.test.sh"
            
            setup: {
                database: true
                external_services: true
                network_access: true
            }
            
            fixtures: {
                database: "./tests/fixtures/database"
                files: "./tests/fixtures/files"
                api_responses: "./tests/fixtures/api"
            }
        }
        
        e2e: {
            description: "End-to-end tests for complete workflows"
            pattern: "**/*.e2e.test.sh"
            
            setup: {
                full_environment: true
                real_services: true
                browser_automation: true
            }
            
            config: {
                base_url: @env.TEST_BASE_URL || "http://localhost:3000"
                browser: "headless-chrome"
                wait_timeout: "10s"
            }
        }
        
        performance: {
            description: "Performance and load tests"
            pattern: "**/*.perf.test.sh"
            
            metrics: {
                response_time: { p95: "200ms", p99: "500ms" }
                throughput: { min: "100rps" }
                memory_usage: { max: "512MB" }
                cpu_usage: { max: "80%" }
            }
            
            load_patterns: {
                ramp_up: { users: 100, duration: "5m" }
                steady: { users: 100, duration: "10m" }
                spike: { users: 500, duration: "2m" }
            }
        }
    }
    
    # Mocking and test doubles
    mocking: {
        # Command mocking
        commands: {
            curl: {
                responses: {
                    "GET https://api.example.com/users": {
                        status: 200,
                        body: @fixture("users.json"),
                        headers: { "Content-Type": "application/json" }
                    },
                    "POST https://api.example.com/users": {
                        status: 201,
                        body: { id: @random.uuid, created: true }
                    }
                }
            }
            
            ssh: {
                responses: {
                    "ssh server 'systemctl status app'": {
                        status: 0,
                        stdout: "● app.service - My Application\n   Active: active (running)"
                    }
                }
            }
        }
        
        # File system mocking
        filesystem: {
            mock_directories: ["/tmp", "/var/log"]
            
            virtual_files: {
                "/etc/config.conf": @fixture("config.conf"),
                "/var/log/app.log": @fixture("app.log")
            }
        }
        
        # Environment mocking
        environment: {
            mock_vars: {
                HOME: "/tmp/mock-home",
                USER: "testuser",
                PATH: "/usr/local/bin:/usr/bin:/bin"
            }
        }
    }
    
    # Test data and fixtures
    fixtures: {
        # Data generators
        generators: {
            random_string: @fn(length = 10) => @random.alphanumeric(length)
            random_email: @fn() => "${@random.word}@example.com"
            random_user: @fn() => {
                id: @random.uuid,
                name: @random.name,
                email: @random.email,
                created_at: @now()
            }
            
            # Faker-style generators
            fake_data: {
                name: ["Alice Johnson", "Bob Smith", "Carol Davis"]
                email: ["alice@test.com", "bob@test.com", "carol@test.com"]
                address: ["123 Test St", "456 Mock Ave", "789 Fake Blvd"]
            }
        }
        
        # Static fixtures
        files: {
            "users.json": [
                { id: 1, name: "Test User 1", email: "user1@test.com" },
                { id: 2, name: "Test User 2", email: "user2@test.com" }
            ]
            
            "config.conf": """
                server.port=3000
                database.host=localhost
                database.name=testdb
            """
            
            "app.log": """
                2024-01-15 10:00:00 INFO Application started
                2024-01-15 10:01:00 DEBUG Processing request
                2024-01-15 10:02:00 ERROR Database connection failed
            """
        }
    }
    
    # Debugging configuration
    debugging: {
        # Debug modes
        modes: {
            trace: {
                enabled: @env.DEBUG_TRACE || false
                output: "./debug/trace.log"
                
                trace_functions: true
                trace_variables: true
                trace_commands: true
            }
            
            step: {
                enabled: @env.DEBUG_STEP || false
                
                breakpoints: ["assert_*", "error_*"]
                interactive: true
            }
            
            profile: {
                enabled: @env.DEBUG_PROFILE || false
                output: "./debug/profile.log"
                
                time_functions: true
                memory_usage: true
                command_stats: true
            }
        }
        
        # Error handling
        error_handling: {
            capture_stack_trace: true
            dump_variables: true
            save_debug_info: true
            
            on_error: {
                action: "debug_shell"
                timeout: "5m"
            }
        }
        
        # Interactive debugging
        interactive: {
            enabled: @env.DEBUG_INTERACTIVE || false
            
            commands: {
                "p": "print_variable"
                "l": "list_variables" 
                "s": "step_next"
                "c": "continue"
                "q": "quit"
                "h": "help"
            }
        }
    }
    
    # Quality assurance
    quality: {
        # Code analysis
        static_analysis: {
            shellcheck: {
                enabled: true
                severity: "warning"
                exclude: ["SC2034", "SC2086"]
            }
            
            custom_rules: [
                {
                    name: "function_naming"
                    pattern: "^[a-z_][a-z0-9_]*$"
                    message: "Function names must be lowercase with underscores"
                },
                {
                    name: "variable_quoting"
                    pattern: "\\$\\{[^}]+\\}"
                    message: "Variables should be quoted: \"$variable\""
                }
            ]
        }
        
        # Performance benchmarks
        benchmarks: {
            function_performance: {
                max_execution_time: "1s"
                memory_limit: "100MB"
                
                critical_functions: [
                    "tusk_parse",
                    "tusk_validate",
                    "tusk_transform"
                ]
            }
        }
        
        # Security checks
        security: {
            command_injection: {
                check_eval: true
                check_unquoted_vars: true
                dangerous_commands: ["rm", "dd", "mkfs"]
            }
            
            secret_detection: {
                patterns: [
                    "password\\s*=\\s*['\"][^'\"]+['\"]",
                    "api_key\\s*=\\s*['\"][^'\"]+['\"]",
                    "secret\\s*=\\s*['\"][^'\"]+['\"]"
                ]
            }
        }
    }
}

# Test suites organization
suites {
    # Core functionality tests
    core: {
        description: "Core TuskLang functionality"
        
        tests: [
            "parser.unit.test.sh",
            "validator.unit.test.sh", 
            "transformer.unit.test.sh",
            "serializer.unit.test.sh"
        ]
        
        dependencies: []
        timeout: "5m"
    }
    
    # Integration tests
    integration: {
        description: "Component integration tests"
        
        tests: [
            "database.integration.test.sh",
            "api.integration.test.sh",
            "file_operations.integration.test.sh"
        ]
        
        dependencies: ["core"]
        timeout: "10m"
        
        setup: {
            database: true
            test_server: true
        }
    }
    
    # End-to-end tests
    e2e: {
        description: "End-to-end user workflows"
        
        tests: [
            "user_registration.e2e.test.sh",
            "config_management.e2e.test.sh",
            "deployment.e2e.test.sh"
        ]
        
        dependencies: ["core", "integration"]
        timeout: "20m"
        
        setup: {
            full_environment: true
            sample_data: true
        }
    }
}
</pre>

<h3>Bash Testing Framework Implementation</h3>
<pre>
#!/bin/bash

# TuskLang Testing Framework

# Global test state
declare -A TEST_STATS
declare -A TEST_CONFIG
declare -a TEST_FAILURES
declare -a TEST_MOCKS
TEST_OUTPUT_DIR="./test-results"
TEST_TEMP_DIR="./tmp/tests"

# Test framework initialization
tusk_test_init() {
    # Initialize stats
    TEST_STATS[total]=0
    TEST_STATS[passed]=0
    TEST_STATS[failed]=0
    TEST_STATS[skipped]=0
    TEST_STATS[start_time]=$(date +%s)
    
    # Create directories
    mkdir -p "$TEST_OUTPUT_DIR" "$TEST_TEMP_DIR"
    
    # Load configuration
    TEST_CONFIG[parallel]="${TEST_PARALLEL:-false}"
    TEST_CONFIG[timeout]="${TEST_TIMEOUT:-30}"
    TEST_CONFIG[debug]="${TEST_DEBUG:-false}"
    
    # Set up test environment
    tusk_test_setup_env
}

# Test environment setup
tusk_test_setup_env() {
    # Backup original environment
    export ORIGINAL_PATH="$PATH"
    export ORIGINAL_HOME="$HOME"
    
    # Set test environment
    export TEST_MODE=true
    export HOME="$TEST_TEMP_DIR/home"
    mkdir -p "$HOME"
    
    # Set up mock paths
    export PATH="$TEST_TEMP_DIR/bin:$ORIGINAL_PATH"
    mkdir -p "$TEST_TEMP_DIR/bin"
}

# Assertion functions
assert_equals() {
    local expected="$1"
    local actual="$2"
    local message="${3:-Values are not equal}"
    
    ((TEST_STATS[total]++))
    
    if [ "$expected" = "$actual" ]; then
        ((TEST_STATS[passed]++))
        tusk_test_log "PASS" "$message"
        return 0
    else
        ((TEST_STATS[failed]++))
        TEST_FAILURES+=("$message: expected '$expected', got '$actual'")
        tusk_test_log "FAIL" "$message: expected '$expected', got '$actual'"
        return 1
    fi
}

assert_not_equals() {
    local expected="$1"
    local actual="$2"
    local message="${3:-Values should not be equal}"
    
    ((TEST_STATS[total]++))
    
    if [ "$expected" != "$actual" ]; then
        ((TEST_STATS[passed]++))
        tusk_test_log "PASS" "$message"
        return 0
    else
        ((TEST_STATS[failed]++))
        TEST_FAILURES+=("$message: both values are '$expected'")
        tusk_test_log "FAIL" "$message: both values are '$expected'"
        return 1
    fi
}

assert_contains() {
    local haystack="$1"
    local needle="$2"
    local message="${3:-String does not contain expected substring}"
    
    ((TEST_STATS[total]++))
    
    if [[ "$haystack" == *"$needle"* ]]; then
        ((TEST_STATS[passed]++))
        tusk_test_log "PASS" "$message"
        return 0
    else
        ((TEST_STATS[failed]++))
        TEST_FAILURES+=("$message: '$haystack' does not contain '$needle'")
        tusk_test_log "FAIL" "$message: '$haystack' does not contain '$needle'"
        return 1
    fi
}

assert_matches() {
    local string="$1"
    local pattern="$2"
    local message="${3:-String does not match pattern}"
    
    ((TEST_STATS[total]++))
    
    if [[ "$string" =~ $pattern ]]; then
        ((TEST_STATS[passed]++))
        tusk_test_log "PASS" "$message"
        return 0
    else
        ((TEST_STATS[failed]++))
        TEST_FAILURES+=("$message: '$string' does not match '$pattern'")
        tusk_test_log "FAIL" "$message: '$string' does not match '$pattern'"
        return 1
    fi
}

assert_true() {
    local condition="$1"
    local message="${2:-Condition is not true}"
    
    ((TEST_STATS[total]++))
    
    if eval "$condition"; then
        ((TEST_STATS[passed]++))
        tusk_test_log "PASS" "$message"
        return 0
    else
        ((TEST_STATS[failed]++))
        TEST_FAILURES+=("$message: condition '$condition' is false")
        tusk_test_log "FAIL" "$message: condition '$condition' is false"
        return 1
    fi
}

assert_false() {
    local condition="$1"
    local message="${2:-Condition is not false}"
    
    ((TEST_STATS[total]++))
    
    if ! eval "$condition"; then
        ((TEST_STATS[passed]++))
        tusk_test_log "PASS" "$message"
        return 0
    else
        ((TEST_STATS[failed]++))
        TEST_FAILURES+=("$message: condition '$condition' is true")
        tusk_test_log "FAIL" "$message: condition '$condition' is true"
        return 1
    fi
}

assert_exits_with() {
    local expected_code="$1"
    local command="$2"
    local message="${3:-Command exit code mismatch}"
    
    ((TEST_STATS[total]++))
    
    local actual_code=0
    eval "$command" >/dev/null 2>&1 || actual_code=$?
    
    if [ "$actual_code" -eq "$expected_code" ]; then
        ((TEST_STATS[passed]++))
        tusk_test_log "PASS" "$message"
        return 0
    else
        ((TEST_STATS[failed]++))
        TEST_FAILURES+=("$message: expected exit code $expected_code, got $actual_code")
        tusk_test_log "FAIL" "$message: expected exit code $expected_code, got $actual_code"
        return 1
    fi
}

assert_output_contains() {
    local expected="$1"
    local command="$2"
    local message="${3:-Command output does not contain expected text}"
    
    ((TEST_STATS[total]++))
    
    local output=$(eval "$command" 2>&1)
    
    if [[ "$output" == *"$expected"* ]]; then
        ((TEST_STATS[passed]++))
        tusk_test_log "PASS" "$message"
        return 0
    else
        ((TEST_STATS[failed]++))
        TEST_FAILURES+=("$message: output '$output' does not contain '$expected'")
        tusk_test_log "FAIL" "$message: output does not contain '$expected'"
        return 1
    fi
}

# Mocking system
mock_command() {
    local command="$1"
    local response="$2"
    local exit_code="${3:-0}"
    
    # Create mock script
    cat > "$TEST_TEMP_DIR/bin/$command" << EOF
#!/bin/bash
echo "$response"
exit $exit_code
EOF
    
    chmod +x "$TEST_TEMP_DIR/bin/$command"
    TEST_MOCKS+=("$command")
    
    tusk_test_log "MOCK" "Command '$command' mocked"
}

mock_file() {
    local file_path="$1"
    local content="$2"
    
    # Create directory if needed
    mkdir -p "$(dirname "$file_path")"
    
    # Create mock file
    echo "$content" > "$file_path"
    
    tusk_test_log "MOCK" "File '$file_path' mocked"
}

# Test discovery and execution
tusk_test_discover() {
    local pattern="${1:-**/*.test.sh}"
    
    find . -name "$pattern" -type f | grep -v node_modules | grep -v vendor
}

tusk_test_run_file() {
    local test_file="$1"
    
    tusk_test_log "INFO" "Running test file: $test_file"
    
    # Source the test file in a subshell
    (
        # Reset stats for this file
        declare -A FILE_STATS
        FILE_STATS[total]=0
        FILE_STATS[passed]=0
        FILE_STATS[failed]=0
        
        # Set up test environment
        cd "$TEST_TEMP_DIR"
        
        # Source test file
        source "$test_file"
        
        # Run setup if it exists
        if declare -f setup >/dev/null; then
            setup
        fi
        
        # Find and run test functions
        local test_functions=$(declare -F | grep -o 'test_[a-zA-Z0-9_]*' || true)
        
        for test_func in $test_functions; do
            tusk_test_log "INFO" "Running test: $test_func"
            
            # Run before_each if it exists
            if declare -f before_each >/dev/null; then
                before_each
            fi
            
            # Run the test
            if $test_func; then
                tusk_test_log "PASS" "$test_func passed"
            else
                tusk_test_log "FAIL" "$test_func failed"
            fi
            
            # Run after_each if it exists
            if declare -f after_each >/dev/null; then
                after_each
            fi
        done
        
        # Run teardown if it exists
        if declare -f teardown >/dev/null; then
            teardown
        fi
    )
}

# Test logging
tusk_test_log() {
    local level="$1"
    local message="$2"
    local timestamp=$(date '+%Y-%m-%d %H:%M:%S')
    
    # Color output
    local color=""
    case "$level" in
        PASS) color="\033[32m" ;;    # green
        FAIL) color="\033[31m" ;;    # red
        SKIP) color="\033[33m" ;;    # yellow
        INFO) color="\033[34m" ;;    # blue
        MOCK) color="\033[35m" ;;    # magenta
    esac
    
    echo -e "${color}[$level]${color:+\033[0m} $message"
    
    # Also write to log file
    echo "[$timestamp] [$level] $message" >> "$TEST_OUTPUT_DIR/test.log"
}

# Test fixtures
tusk_test_fixture() {
    local fixture_name="$1"
    local fixture_file="./tests/fixtures/$fixture_name"
    
    if [ -f "$fixture_file" ]; then
        cat "$fixture_file"
    else
        tusk_test_log "FAIL" "Fixture not found: $fixture_name"
        return 1
    fi
}

# Random data generators
tusk_test_random_string() {
    local length="${1:-10}"
    openssl rand -base64 32 | tr -d '/+=' | head -c "$length"
}

tusk_test_random_email() {
    local username=$(tusk_test_random_string 8)
    echo "${username}@test.com"
}

tusk_test_random_uuid() {
    uuidgen 2>/dev/null || echo "$(openssl rand -hex 16 | sed 's/\(..\)/\1/g' | head -c 32)"
}

# Performance testing
tusk_test_benchmark() {
    local test_name="$1"
    local command="$2"
    local iterations="${3:-10}"
    
    tusk_test_log "INFO" "Benchmarking: $test_name"
    
    local total_time=0
    local min_time=999999
    local max_time=0
    
    for ((i=1; i<=iterations; i++)); do
        local start_time=$(date +%s%N)
        eval "$command" >/dev/null 2>&1
        local end_time=$(date +%s%N)
        
        local duration=$((end_time - start_time))
        total_time=$((total_time + duration))
        
        [ $duration -lt $min_time ] && min_time=$duration
        [ $duration -gt $max_time ] && max_time=$duration
    done
    
    local avg_time=$((total_time / iterations))
    
    # Convert nanoseconds to milliseconds
    local avg_ms=$((avg_time / 1000000))
    local min_ms=$((min_time / 1000000))
    local max_ms=$((max_time / 1000000))
    
    tusk_test_log "INFO" "Benchmark results: avg=${avg_ms}ms, min=${min_ms}ms, max=${max_ms}ms"
    
    # Write to benchmark file
    echo "$test_name,$avg_ms,$min_ms,$max_ms" >> "$TEST_OUTPUT_DIR/benchmarks.csv"
}

# Test reporting
tusk_test_generate_report() {
    local end_time=$(date +%s)
    local duration=$((end_time - ${TEST_STATS[start_time]}))
    
    # Console report
    echo
    echo "============================================"
    echo "            TEST RESULTS"
    echo "============================================"
    echo "Total tests:  ${TEST_STATS[total]}"
    echo "Passed:       ${TEST_STATS[passed]}"
    echo "Failed:       ${TEST_STATS[failed]}"
    echo "Skipped:      ${TEST_STATS[skipped]}"
    echo "Duration:     ${duration}s"
    echo
    
    if [ ${TEST_STATS[failed]} -gt 0 ]; then
        echo "FAILURES:"
        for failure in "${TEST_FAILURES[@]}"; do
            echo "  ✗ $failure"
        done
        echo
    fi
    
    # Success rate
    if [ ${TEST_STATS[total]} -gt 0 ]; then
        local success_rate=$(( (TEST_STATS[passed] * 100) / TEST_STATS[total] ))
        echo "Success rate: ${success_rate}%"
    fi
    
    # JUnit XML report
    tusk_test_generate_junit_report
    
    # JSON report
    tusk_test_generate_json_report
}

tusk_test_generate_junit_report() {
    local junit_file="$TEST_OUTPUT_DIR/junit.xml"
    
    cat > "$junit_file" << EOF
<?xml version="1.0" encoding="UTF-8"?>
<testsuites tests="${TEST_STATS[total]}" failures="${TEST_STATS[failed]}" time="$(($(date +%s) - ${TEST_STATS[start_time]}))">
  <testsuite name="TuskLang Tests" tests="${TEST_STATS[total]}" failures="${TEST_STATS[failed]}" time="$(($(date +%s) - ${TEST_STATS[start_time]}))">
EOF
    
    # Add test cases (simplified)
    for ((i=1; i<=${TEST_STATS[total]}; i++)); do
        if [ $i -le ${TEST_STATS[passed]} ]; then
            echo "    <testcase name=\"test_$i\" classname=\"TuskLang\" time=\"0.1\"/>" >> "$junit_file"
        else
            echo "    <testcase name=\"test_$i\" classname=\"TuskLang\" time=\"0.1\">" >> "$junit_file"
            echo "      <failure message=\"Test failed\">Test assertion failed</failure>" >> "$junit_file"
            echo "    </testcase>" >> "$junit_file"
        fi
    done
    
    cat >> "$junit_file" << EOF
  </testsuite>
</testsuites>
EOF
}

tusk_test_generate_json_report() {
    local json_file="$TEST_OUTPUT_DIR/report.json"
    
    cat > "$json_file" << EOF
{
    "summary": {
        "total": ${TEST_STATS[total]},
        "passed": ${TEST_STATS[passed]},
        "failed": ${TEST_STATS[failed]},
        "skipped": ${TEST_STATS[skipped]},
        "duration": $(($(date +%s) - ${TEST_STATS[start_time]})),
        "success_rate": $(( TEST_STATS[total] > 0 ? (TEST_STATS[passed] * 100) / TEST_STATS[total] : 0 ))
    },
    "failures": [
$(IFS=$'\n'; echo "${TEST_FAILURES[*]}" | sed 's/^/        "/' | sed 's/$/",/' | sed '$s/,$//')
    ],
    "timestamp": "$(date -u +%Y-%m-%dT%H:%M:%SZ)"
}
EOF
}

# Debug mode
tusk_test_debug() {
    local command="$1"
    
    if [ "${TEST_CONFIG[debug]}" = "true" ]; then
        echo "DEBUG: $command" >&2
        echo "Press Enter to continue, 'q' to quit, 's' to step through..."
        read -r response
        
        case "$response" in
            q) exit 1 ;;
            s) set -x ;;
        esac
    fi
}

# Test runner main function
tusk_test_run() {
    local pattern="${1:-**/*.test.sh}"
    
    tusk_test_init
    
    tusk_test_log "INFO" "Starting test suite"
    tusk_test_log "INFO" "Pattern: $pattern"
    
    # Discover tests
    local test_files=($(tusk_test_discover "$pattern"))
    tusk_test_log "INFO" "Found ${#test_files[@]} test files"
    
    # Run tests
    for test_file in "${test_files[@]}"; do
        tusk_test_run_file "$test_file"
    done
    
    # Generate reports
    tusk_test_generate_report
    
    # Cleanup
    tusk_test_cleanup
    
    # Exit with appropriate code
    [ ${TEST_STATS[failed]} -eq 0 ] && exit 0 || exit 1
}

# Cleanup
tusk_test_cleanup() {
    # Restore environment
    export PATH="$ORIGINAL_PATH"
    export HOME="$ORIGINAL_HOME"
    
    # Clean up mocks
    for mock in "${TEST_MOCKS[@]}"; do
        rm -f "$TEST_TEMP_DIR/bin/$mock"
    done
    
    # Clean up temp directory
    rm -rf "$TEST_TEMP_DIR"
    
    tusk_test_log "INFO" "Test cleanup completed"
}
</pre>

<h3>Usage Example</h3>
<pre>
#!/bin/bash

# Load testing framework
source /usr/local/lib/tusklang-testing.sh

echo "=== Testing & Debugging Demo ==="

# 1. Create sample functions to test
echo -e "\n1. Setting up test subjects:"

# Simple function to test
add_numbers() {
    local a="$1"
    local b="$2"
    echo $((a + b))
}

# Function with external dependency
get_user_info() {
    local user_id="$1"
    curl -s "https://api.example.com/users/$user_id"
}

# Function that manipulates files
create_config() {
    local file="$1"
    local env="${2:-development}"
    
    cat > "$file" << EOF
environment=$env
debug=true
log_level=info
EOF
}

echo "Test subjects created: add_numbers, get_user_info, create_config"

# 2. Create test files
echo -e "\n2. Creating test files:"

mkdir -p tests

# Unit test file
cat > tests/math.unit.test.sh << 'EOF'
#!/bin/bash

# Unit tests for math functions

test_add_numbers_positive() {
    local result=$(add_numbers 5 3)
    assert_equals "8" "$result" "5 + 3 should equal 8"
}

test_add_numbers_negative() {
    local result=$(add_numbers -2 4)
    assert_equals "2" "$result" "-2 + 4 should equal 2"
}

test_add_numbers_zero() {
    local result=$(add_numbers 0 0)
    assert_equals "0" "$result" "0 + 0 should equal 0"
}

test_add_numbers_large() {
    local result=$(add_numbers 1000 2000)
    assert_equals "3000" "$result" "1000 + 2000 should equal 3000"
}
EOF

# Integration test with mocking
cat > tests/api.integration.test.sh << 'EOF'
#!/bin/bash

# Integration tests with mocking

setup() {
    # Mock the curl command
    mock_command "curl" '{"id":123,"name":"Test User","email":"test@example.com"}'
}

test_get_user_info_success() {
    local result=$(get_user_info 123)
    assert_contains "$result" "Test User" "Should return user information"
    assert_contains "$result" "test@example.com" "Should contain email"
}

test_get_user_info_with_id() {
    local result=$(get_user_info 456)
    assert_contains "$result" "Test User" "Should work with any user ID"
}

teardown() {
    # Cleanup is handled automatically
    echo "Test teardown completed"
}
EOF

# File operations test
cat > tests/config.unit.test.sh << 'EOF'
#!/bin/bash

# Tests for configuration functions

before_each() {
    # Create a temp directory for each test
    test_config_dir="$TEST_TEMP_DIR/config_test_$$"
    mkdir -p "$test_config_dir"
}

after_each() {
    # Clean up after each test
    rm -rf "$test_config_dir"
}

test_create_config_default() {
    local config_file="$test_config_dir/app.conf"
    create_config "$config_file"
    
    assert_true "[ -f '$config_file' ]" "Config file should be created"
    assert_output_contains "environment=development" "cat '$config_file'"
}

test_create_config_production() {
    local config_file="$test_config_dir/prod.conf"
    create_config "$config_file" "production"
    
    assert_output_contains "environment=production" "cat '$config_file'"
}

test_create_config_directory_creation() {
    local config_file="$test_config_dir/nested/deep/config.conf"
    create_config "$config_file"
    
    assert_true "[ -f '$config_file' ]" "Should create nested directories"
}
EOF

echo "Created test files: math.unit.test.sh, api.integration.test.sh, config.unit.test.sh"

# 3. Run individual test demonstrations
echo -e "\n3. Running individual assertions:"

# Test various assertion types
echo "Testing assertions:"

assert_equals "hello" "hello" "String equality test"
assert_not_equals "foo" "bar" "String inequality test"
assert_contains "hello world" "world" "String contains test"
assert_matches "test123" "^test[0-9]+$" "Regex match test"
assert_true "[ 5 -gt 3 ]" "Greater than test"
assert_false "[ 2 -gt 5 ]" "Less than test"

# 4. Mock command demonstration
echo -e "\n4. Command mocking demonstration:"

# Mock a command
mock_command "date" "2024-01-15 12:00:00"

echo "Original date command mocked:"
date

# Mock with exit code
mock_command "failing_command" "Error occurred" 1

echo "Testing command that should fail:"
if failing_command; then
    echo "Command succeeded unexpectedly"
else
    echo "Command failed as expected (exit code: $?)"
fi

# 5. File mocking
echo -e "\n5. File mocking demonstration:"

mock_file "/tmp/test-config.conf" "server.port=8080
database.host=localhost
debug=true"

echo "Mocked file contents:"
cat /tmp/test-config.conf

# 6. Test discovery and execution
echo -e "\n6. Test execution demonstration:"

# Initialize the test framework
tusk_test_init

echo "Running math unit tests:"
tusk_test_run_file "tests/math.unit.test.sh"

echo -e "\nRunning API integration tests:"
tusk_test_run_file "tests/api.integration.test.sh"

echo -e "\nRunning config unit tests:"
tusk_test_run_file "tests/config.unit.test.sh"

# 7. Performance benchmarking
echo -e "\n7. Performance benchmarking:"

# Create a function to benchmark
slow_function() {
    sleep 0.1
    echo "Processing complete"
}

fast_function() {
    echo "Quick operation"
}

echo "Benchmarking slow function (10 iterations):"
tusk_test_benchmark "slow_function" "slow_function" 3

echo "Benchmarking fast function (10 iterations):"
tusk_test_benchmark "fast_function" "fast_function" 5

# 8. Random data generation
echo -e "\n8. Test data generation:"

echo "Random string: $(tusk_test_random_string 12)"
echo "Random email: $(tusk_test_random_email)"
echo "Random UUID: $(tusk_test_random_uuid)"

# Generate test data
for i in {1..3}; do
    echo "Test user $i: $(tusk_test_random_string 8)@test.com"
done

# 9. Test fixtures
echo -e "\n9. Test fixtures:"

# Create fixture directory and files
mkdir -p tests/fixtures

cat > tests/fixtures/users.json << 'EOF'
[
    {"id": 1, "name": "Alice", "email": "alice@test.com"},
    {"id": 2, "name": "Bob", "email": "bob@test.com"},
    {"id": 3, "name": "Carol", "email": "carol@test.com"}
]
EOF

cat > tests/fixtures/config.conf << 'EOF'
# Test configuration
server.port=3000
database.url=postgresql://localhost/testdb
redis.url=redis://localhost:6379
EOF

echo "Loading fixture data:"
echo "Users fixture:"
tusk_test_fixture "users.json" | head -3

echo -e "\nConfig fixture:"
tusk_test_fixture "config.conf" | head -3

# 10. Debugging demonstration
echo -e "\n10. Debugging features:"

# Debug mode demonstration
TEST_CONFIG[debug]=false  # Set to true for interactive debugging

debug_function() {
    local input="$1"
    tusk_test_debug "Processing input: $input"
    
    local result=$(echo "$input" | tr '[:lower:]' '[:upper:]')
    tusk_test_debug "Result: $result"
    
    echo "$result"
}

echo "Debug function example:"
result=$(debug_function "hello world")
echo "Final result: $result"

# 11. Test reporting
echo -e "\n11. Test reports generation:"

# Generate test reports
tusk_test_generate_report

echo "Generated test reports:"
ls -la "$TEST_OUTPUT_DIR"

if [ -f "$TEST_OUTPUT_DIR/report.json" ]; then
    echo -e "\nJSON report sample:"
    head -10 "$TEST_OUTPUT_DIR/report.json"
fi

# 12. Coverage analysis simulation
echo -e "\n12. Code coverage simulation:"

# Simulate coverage analysis
covered_lines=45
total_lines=60
coverage_percent=$(( (covered_lines * 100) / total_lines ))

echo "Code coverage analysis:"
echo "  Lines covered: $covered_lines"
echo "  Total lines: $total_lines" 
echo "  Coverage: ${coverage_percent}%"

if [ $coverage_percent -ge 80 ]; then
    echo "  ✓ Coverage threshold met (≥80%)"
else
    echo "  ✗ Coverage below threshold (<80%)"
fi

# 13. Error simulation and testing
echo -e "\n13. Error handling tests:"

error_function() {
    local should_fail="$1"
    
    if [ "$should_fail" = "true" ]; then
        echo "Error: Something went wrong" >&2
        return 1
    else
        echo "Success"
        return 0
    fi
}

# Test successful case
assert_exits_with 0 "error_function false" "Function should succeed with false"

# Test failure case
assert_exits_with 1 "error_function true" "Function should fail with true"

# Test output
assert_output_contains "Success" "error_function false" "Should output success message"

# Final statistics
echo -e "\n14. Final test statistics:"
tusk_test_generate_report

# Cleanup
echo -e "\n15. Cleanup:"
tusk_test_cleanup
rm -rf tests

echo -e "\n=== Testing demonstration complete ==="
</pre>

<p>TuskLang testing and debugging framework in bash provides comprehensive testing capabilities including unit tests, integration tests, mocking, fixtures, benchmarking, and detailed reporting for robust quality assurance.</p>